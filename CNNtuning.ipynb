{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 17:06:26.916686: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-28 17:06:27.852199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"original_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11237 images belonging to 9 classes.\n",
      "Found 1242 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_ratio = 0.1\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_data_gen = ImageDataGenerator(rescale=1.0/255, validation_split=validation_ratio,zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest')\n",
    "train_generator = train_data_gen.flow_from_directory(data_path, target_size=(224,224), batch_size=4, class_mode='categorical', subset='training')\n",
    "valid_generator = train_data_gen.flow_from_directory(data_path, target_size=(224,224), batch_size=4, class_mode='categorical', subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : black_spot\n",
      "1 : black_spot_early\n",
      "2 : canker\n",
      "3 : canker_early\n",
      "4 : greening\n",
      "5 : greening_early\n",
      "6 : healthy\n",
      "7 : melanose\n",
      "8 : melanose_early\n"
     ]
    }
   ],
   "source": [
    "labels = {value: key for key, value in train_generator.class_indices.items()}\n",
    "for key, value in labels.items():\n",
    "    print(f\"{key} : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=128, kernel_size=(5, 5), padding='valid', input_shape=(224, 224, 3)),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Conv2D(filters=64, kernel_size=(3, 3), padding='valid', kernel_regularizer=l2(0.00005)),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Conv2D(filters=32, kernel_size=(3, 3), padding='valid', kernel_regularizer=l2(0.00005)),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Flatten(),\n",
    "        \n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(units=6, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 17:06:41.226042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 73 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 220, 220, 128)     9728      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 220, 220, 128)     0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 110, 110, 128)     0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 110, 110, 128)     512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 108, 108, 64)      73792     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 108, 108, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 54, 54, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 32)        18464     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 52, 52, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 26, 26, 32)        128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 21632)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               5538048   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5642470 (21.52 MB)\n",
      "Trainable params: 5642022 (21.52 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cnn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=np.sqrt(0.1), patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = Adam(learning_rate=0.001)\n",
    "cnn_model.compile(optimizer=opt1, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-28 17:07:06.542312: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 21.12MiB (rounded to 22151168)requested by op Fill\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-11-28 17:07:06.542364: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-11-28 17:07:06.542386: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 42, Chunks in use: 42. 10.5KiB allocated for chunks. 10.5KiB in use in bin. 4.2KiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542402: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 11, Chunks in use: 11. 5.5KiB allocated for chunks. 5.5KiB in use in bin. 5.5KiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542418: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 2, Chunks in use: 2. 2.2KiB allocated for chunks. 2.2KiB in use in bin. 2.0KiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542433: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542449: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 2, Chunks in use: 1. 11.0KiB allocated for chunks. 6.0KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542463: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542476: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542492: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 3, Chunks in use: 3. 123.5KiB allocated for chunks. 123.5KiB in use in bin. 112.5KiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542509: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 3, Chunks in use: 3. 250.5KiB allocated for chunks. 250.5KiB in use in bin. 216.0KiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542522: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542537: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 3, Chunks in use: 3. 936.0KiB allocated for chunks. 936.0KiB in use in bin. 864.0KiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542550: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542562: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542575: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542587: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542599: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542615: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 1, Chunks in use: 1. 29.79MiB allocated for chunks. 29.79MiB in use in bin. 21.12MiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542632: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 1. 41.90MiB allocated for chunks. 41.90MiB in use in bin. 21.12MiB client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542645: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542658: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542670: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-11-28 17:07:06.542683: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 21.12MiB was 16.00MiB, Chunk State: \n",
      "2023-11-28 17:07:06.542695: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 76546048\n",
      "2023-11-28 17:07:06.542709: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000000 of size 256 next 1\n",
      "2023-11-28 17:07:06.542720: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000100 of size 1280 next 2\n",
      "2023-11-28 17:07:06.542730: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000600 of size 256 next 3\n",
      "2023-11-28 17:07:06.542739: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000700 of size 256 next 4\n",
      "2023-11-28 17:07:06.542749: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000800 of size 256 next 6\n",
      "2023-11-28 17:07:06.542759: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000900 of size 512 next 7\n",
      "2023-11-28 17:07:06.542769: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000b00 of size 256 next 5\n",
      "2023-11-28 17:07:06.542779: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000c00 of size 512 next 8\n",
      "2023-11-28 17:07:06.542789: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702000e00 of size 512 next 11\n",
      "2023-11-28 17:07:06.542798: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001000 of size 512 next 12\n",
      "2023-11-28 17:07:06.542808: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001200 of size 512 next 13\n",
      "2023-11-28 17:07:06.542817: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001400 of size 256 next 14\n",
      "2023-11-28 17:07:06.542827: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001500 of size 256 next 15\n",
      "2023-11-28 17:07:06.542836: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001600 of size 256 next 18\n",
      "2023-11-28 17:07:06.542846: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001700 of size 256 next 16\n",
      "2023-11-28 17:07:06.542856: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001800 of size 256 next 17\n",
      "2023-11-28 17:07:06.542865: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001900 of size 256 next 21\n",
      "2023-11-28 17:07:06.542875: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001a00 of size 256 next 22\n",
      "2023-11-28 17:07:06.542884: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001b00 of size 256 next 23\n",
      "2023-11-28 17:07:06.542894: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001c00 of size 256 next 24\n",
      "2023-11-28 17:07:06.542903: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001d00 of size 256 next 27\n",
      "2023-11-28 17:07:06.542913: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001e00 of size 256 next 25\n",
      "2023-11-28 17:07:06.542923: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702001f00 of size 256 next 26\n",
      "2023-11-28 17:07:06.542932: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002000 of size 256 next 30\n",
      "2023-11-28 17:07:06.542942: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002100 of size 256 next 31\n",
      "2023-11-28 17:07:06.542951: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002200 of size 256 next 32\n",
      "2023-11-28 17:07:06.542965: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002300 of size 256 next 33\n",
      "2023-11-28 17:07:06.542976: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002400 of size 1024 next 36\n",
      "2023-11-28 17:07:06.542986: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002800 of size 256 next 34\n",
      "2023-11-28 17:07:06.542996: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002900 of size 256 next 35\n",
      "2023-11-28 17:07:06.543005: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002a00 of size 256 next 39\n",
      "2023-11-28 17:07:06.543015: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002b00 of size 256 next 40\n",
      "2023-11-28 17:07:06.543024: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002c00 of size 256 next 37\n",
      "2023-11-28 17:07:06.543034: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002d00 of size 256 next 41\n",
      "2023-11-28 17:07:06.543044: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002e00 of size 256 next 44\n",
      "2023-11-28 17:07:06.543053: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702002f00 of size 256 next 45\n",
      "2023-11-28 17:07:06.543063: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003000 of size 256 next 46\n",
      "2023-11-28 17:07:06.543072: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003100 of size 256 next 47\n",
      "2023-11-28 17:07:06.543082: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003200 of size 512 next 49\n",
      "2023-11-28 17:07:06.543092: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003400 of size 512 next 50\n",
      "2023-11-28 17:07:06.543101: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003600 of size 512 next 51\n",
      "2023-11-28 17:07:06.543111: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003800 of size 512 next 52\n",
      "2023-11-28 17:07:06.543120: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003a00 of size 512 next 53\n",
      "2023-11-28 17:07:06.543130: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003c00 of size 512 next 54\n",
      "2023-11-28 17:07:06.543140: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003e00 of size 256 next 56\n",
      "2023-11-28 17:07:06.543150: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702003f00 of size 256 next 57\n",
      "2023-11-28 17:07:06.543159: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004000 of size 256 next 58\n",
      "2023-11-28 17:07:06.543169: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004100 of size 256 next 59\n",
      "2023-11-28 17:07:06.543179: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004200 of size 256 next 60\n",
      "2023-11-28 17:07:06.543189: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004300 of size 256 next 62\n",
      "2023-11-28 17:07:06.543199: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004400 of size 256 next 63\n",
      "2023-11-28 17:07:06.543209: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004500 of size 256 next 64\n",
      "2023-11-28 17:07:06.543219: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004600 of size 256 next 65\n",
      "2023-11-28 17:07:06.543230: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004700 of size 256 next 66\n",
      "2023-11-28 17:07:06.543239: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702004800 of size 256 next 67\n",
      "2023-11-28 17:07:06.543250: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7ff702004900 of size 5120 next 42\n",
      "2023-11-28 17:07:06.543260: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702005d00 of size 6144 next 43\n",
      "2023-11-28 17:07:06.543270: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702007500 of size 49664 next 9\n",
      "2023-11-28 17:07:06.543281: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702013700 of size 38400 next 10\n",
      "2023-11-28 17:07:06.543292: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff70201cd00 of size 38400 next 48\n",
      "2023-11-28 17:07:06.543303: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702026300 of size 109056 next 29\n",
      "2023-11-28 17:07:06.543313: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702040d00 of size 73728 next 28\n",
      "2023-11-28 17:07:06.543324: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff702052d00 of size 368640 next 20\n",
      "2023-11-28 17:07:06.543335: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff7020acd00 of size 294912 next 19\n",
      "2023-11-28 17:07:06.543345: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff7020f4d00 of size 294912 next 55\n",
      "2023-11-28 17:07:06.543355: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff70213cd00 of size 73728 next 61\n",
      "2023-11-28 17:07:06.543366: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff70214ed00 of size 43933696 next 38\n",
      "2023-11-28 17:07:06.543377: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7ff704b34d00 of size 31240960 next 18446744073709551615\n",
      "2023-11-28 17:07:06.543389: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-11-28 17:07:06.543404: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 42 Chunks of size 256 totalling 10.5KiB\n",
      "2023-11-28 17:07:06.543417: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 11 Chunks of size 512 totalling 5.5KiB\n",
      "2023-11-28 17:07:06.543429: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1024 totalling 1.0KiB\n",
      "2023-11-28 17:07:06.543442: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-11-28 17:07:06.543455: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 6144 totalling 6.0KiB\n",
      "2023-11-28 17:07:06.543468: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 38400 totalling 75.0KiB\n",
      "2023-11-28 17:07:06.543481: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 49664 totalling 48.5KiB\n",
      "2023-11-28 17:07:06.543494: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 73728 totalling 144.0KiB\n",
      "2023-11-28 17:07:06.543507: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 109056 totalling 106.5KiB\n",
      "2023-11-28 17:07:06.543520: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 294912 totalling 576.0KiB\n",
      "2023-11-28 17:07:06.543533: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 368640 totalling 360.0KiB\n",
      "2023-11-28 17:07:06.543546: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 31240960 totalling 29.79MiB\n",
      "2023-11-28 17:07:06.543559: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 43933696 totalling 41.90MiB\n",
      "2023-11-28 17:07:06.543572: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 73.00MiB\n",
      "2023-11-28 17:07:06.543590: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 76546048 memory_limit_: 76546048 available bytes: 0 curr_region_allocation_bytes_: 153092096\n",
      "2023-11-28 17:07:06.543608: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                        76546048\n",
      "InUse:                        76540928\n",
      "MaxInUse:                     76540928\n",
      "NumAllocs:                          94\n",
      "MaxAllocSize:                 43933696\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-11-28 17:07:06.543627: W tensorflow/tsl/framework/bfc_allocator.cc:497] *******************************xxxxxxxxxxxxxxxxxxxxxxxxxxxx******************************xxxxxxxxxxx\n",
      "2023-11-28 17:07:06.543659: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at constant_op.cc:175 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[21632,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1084, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 1230, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 638, in apply_gradients\n        self.build(trainable_variables)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/adam.py\", line 148, in build\n        self.add_variable_from_reference(\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 1135, in add_variable_from_reference\n        return super().add_variable_from_reference(\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 508, in add_variable_from_reference\n        initial_value = tf.zeros(\n\n    ResourceExhaustedError: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[21632,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill] name: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m/home/tower1/GitHub/citrus/CNNtuning.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tower1/GitHub/citrus/CNNtuning.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m hist1 \u001b[39m=\u001b[39m cnn_model\u001b[39m.\u001b[39;49mfit(train_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mvalid_generator, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[reduce_lr])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/mambaforge/envs/citrus/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:52\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m     53\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: in user code:\n\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1084, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 544, in minimize\n        self.apply_gradients(grads_and_vars)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 1230, in apply_gradients\n        return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 638, in apply_gradients\n        self.build(trainable_variables)\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/adam.py\", line 148, in build\n        self.add_variable_from_reference(\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 1135, in add_variable_from_reference\n        return super().add_variable_from_reference(\n    File \"/home/tower1/.local/lib/python3.8/site-packages/keras/src/optimizers/optimizer.py\", line 508, in add_variable_from_reference\n        initial_value = tf.zeros(\n\n    ResourceExhaustedError: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[21632,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill] name: \n"
     ]
    }
   ],
   "source": [
    "hist1 = cnn_model.fit(train_generator, epochs=30, validation_data=valid_generator, verbose=2, callbacks=[reduce_lr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
